"""
LLMç®¡ç†å™¨æ¨¡å—

è´Ÿè´£ç®¡ç†å’Œåˆ›å»ºä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹å®ä¾‹ï¼Œä¸»è¦æ”¯æŒGoogle Geminiå’ŒOllamaæœ¬åœ°æ¨¡å‹ã€‚
é›†æˆäº†Gemini APIè¯Šæ–­å’Œç›‘æ§åŠŸèƒ½ã€‚
"""

import logging
import os
import time
from typing import Dict, Any, Optional, List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models import ChatOllama
from .gemini_diagnostics import get_global_diagnostics

logger = logging.getLogger(__name__)


class LLMManager:
    """LLMç®¡ç†å™¨ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMç®¡ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.llm_cache = {}  # LLMå®ä¾‹ç¼“å­˜
        
        # æ”¯æŒçš„æ¨¡å‹é…ç½®
        self.supported_models = {
            # Google Gemini 1.5ç³»åˆ—æ¨¡å‹ï¼ˆæ›´ç¨³å®šï¼‰
            "gemini-1.5-pro": {
                "provider": "gemini",
                "model_name": "gemini-1.5-pro",
                "temperature": 0.1,
                "max_tokens": 8192
            },
            "gemini-1.5-flash": {
                "provider": "gemini",
                "model_name": "gemini-1.5-flash",
                "temperature": 0.1,
                "max_tokens": 8192  # 1.5 Flashç¨³å®šç‰ˆæœ¬ï¼Œé€‚ä¸­çš„tokené™åˆ¶
            },
            
            # Google Gemini 2.5ç³»åˆ—æ¨¡å‹ï¼ˆå¯èƒ½ä¸ç¨³å®šï¼‰
            "gemini-2.5-pro": {
                "provider": "gemini",
                "model_name": "gemini-2.5-pro",
                "temperature": 0.1,
                "max_tokens": 8192
            },
            "gemini-2.5-flash": {
                "provider": "gemini",
                "model_name": "gemini-2.5-flash",
                "temperature": 0.1,
                "max_tokens": 32768  # å†æ¬¡å¢åŠ ï¼šä»16384å¢åŠ åˆ°32768æ”¯æŒå®Œæ•´è¶‹åŠ¿åˆ†ææŠ¥å‘Š
            },
            
            # Ollamaæœ¬åœ°æ¨¡å‹
            "llama3": {
                "provider": "ollama",
                "model_name": "llama3",
                "temperature": 0.1,
                "max_tokens": 4096
            },
            "qwen": {
                "provider": "ollama",
                "model_name": "qwen",
                "temperature": 0.1,
                "max_tokens": 4096
            }
        }
    
    def get_llm(self, model_name: str, **kwargs) -> Any:
        """
        è·å–LLMå®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            **kwargs: é¢å¤–çš„æ¨¡å‹å‚æ•°
            
        Returns:
            LLMå®ä¾‹
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{model_name}_{hash(str(sorted(kwargs.items())))}"
            if cache_key in self.llm_cache:
                return self.llm_cache[cache_key]
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
            if model_name not in self.supported_models:
                logger.warning(f"Model {model_name} not in supported models, using default gemini-2.5-flash")
                model_name = "gemini-2.5-flash"
            
            model_config = self.supported_models[model_name].copy()
            model_config.update(kwargs)  # ç”¨ä¼ å…¥çš„å‚æ•°è¦†ç›–é»˜è®¤é…ç½®
            
            provider = model_config["provider"]
            
            # åˆ›å»ºLLMå®ä¾‹
            if provider == "gemini":
                llm = self._create_gemini_llm(model_config)
            elif provider == "ollama":
                llm = self._create_ollama_llm(model_config)
            else:
                raise ValueError(f"Unsupported provider: {provider}")
            
            # ç¼“å­˜å®ä¾‹
            self.llm_cache[cache_key] = llm
            
            logger.info(f"Created LLM instance: {model_name}")
            return llm
            
        except Exception as e:
            logger.error(f"Error creating LLM {model_name}: {str(e)}")
            raise
    
    def _create_gemini_llm(self, config: Dict[str, Any]) -> ChatGoogleGenerativeAI:
        """åˆ›å»ºGoogle Gemini LLMå®ä¾‹"""
        # åŸºç¡€é…ç½® - ç®€åŒ–ä½†æœ‰æ•ˆçš„ä¼˜åŒ–
        gemini_config = {
            "model": config["model_name"],
            "temperature": config["temperature"],
            "max_output_tokens": config["max_tokens"],
            "timeout": 300,  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œåº”å¯¹å¤æ‚è¯·æ±‚
            "max_retries": 5,  # å¢åŠ é‡è¯•æ¬¡æ•°åº”å¯¹500é”™è¯¯
        }
        
        # æŒ‰ä¼˜å…ˆçº§è·å–APIå¯†é’¥
        api_key = (
            os.getenv("GOOGLE_API_KEY") or 
            os.getenv("GEMINI_API_KEY") or
            self.config.get("google_api_key") or 
            self.config.get("gemini_api_key")
        )
        
        if not api_key:
            logger.error("Google API key not found in environment variables or config")
            logger.error("Please set one of: GOOGLE_API_KEY, GEMINI_API_KEY")
            raise ValueError("Missing Google API key. Please set GOOGLE_API_KEY environment variable.")
        
        # æ˜¾å¼è®¾ç½®APIå¯†é’¥
        gemini_config["google_api_key"] = api_key
        
        # è®°å½•æ¨¡å‹é…ç½®ä¿¡æ¯
        logger.info(f"Creating Gemini LLM with model: {config['model_name']}")
        
        try:
            return ChatGoogleGenerativeAI(**gemini_config)
        except Exception as e:
            logger.error(f"Failed to create Gemini LLM: {str(e)}")
            logger.error("Please verify your GOOGLE_API_KEY is valid and has access to Gemini models")
            raise
    
    def _wrap_gemini_with_error_handling(self, llm_instance):
        """ä¸ºGemini LLMæ·»åŠ ç‰¹æ®Šçš„é”™è¯¯å¤„ç†åŒ…è£…ï¼Œé›†æˆè¯Šæ–­ç³»ç»Ÿ"""
        from functools import wraps
        
        original_invoke = llm_instance.invoke
        diagnostics = get_global_diagnostics()
        
        @wraps(original_invoke)
        def enhanced_invoke(*args, **kwargs):
            max_attempts = 3
            base_delay = 5  # åŸºç¡€å»¶è¿Ÿ5ç§’
            start_time = time.time()
            
            # è®¡ç®—è¯·æ±‚å¤§å°
            request_size = 0
            if args:
                request_size = len(str(args[0])) if args[0] else 0
            
            for attempt in range(max_attempts):
                try:
                    result = original_invoke(*args, **kwargs)
                    
                    # è®°å½•æˆåŠŸè¯·æ±‚
                    response_time = time.time() - start_time
                    diagnostics.record_success(
                        model_name=getattr(llm_instance, 'model_name', 'gemini'),
                        response_time=response_time
                    )
                    
                    logger.info(f"âœ… Gemini APIè°ƒç”¨æˆåŠŸ (å“åº”æ—¶é—´: {response_time:.2f}s)")
                    return result
                    
                except Exception as e:
                    error_msg = str(e).lower()
                    response_time = time.time() - start_time
                    
                    # è®°å½•é”™è¯¯åˆ°è¯Šæ–­ç³»ç»Ÿ
                    error_type = diagnostics.record_error(
                        error=e,
                        model_name=getattr(llm_instance, 'model_name', 'gemini'),
                        request_size=request_size,
                        response_time=response_time,
                        retry_count=attempt + 1
                    )
                    
                    # ä¸“é—¨å¤„ç†500é”™è¯¯
                    if "500" in error_msg or "internal error" in error_msg:
                        if attempt < max_attempts - 1:
                            delay = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                            logger.warning(f"ğŸ”„ Gemini 500é”™è¯¯ï¼Œ{delay}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_attempts})")
                            logger.warning(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
                            time.sleep(delay)
                            continue
                        else:
                            logger.error("âŒ Gemini 500é”™è¯¯é‡è¯•æ¬¡æ•°å·²ç”¨å°½")
                            # ç”Ÿæˆè¯Šæ–­å»ºè®®
                            health = diagnostics.get_health_status()
                            recent_analysis = diagnostics.analyze_recent_errors(hours=1)
                            
                            logger.error(f"ç³»ç»Ÿå¥åº·çŠ¶æ€: {health.get('status', 'unknown')} (æˆåŠŸç‡: {health.get('success_rate', 0):.1f}%)")
                            
                            if recent_analysis.get('recommendations'):
                                logger.error("å»ºè®®æªæ–½:")
                                for rec in recent_analysis['recommendations'][:3]:
                                    logger.error(f"  - {rec}")
                            
                            raise Exception(f"Gemini APIæŒç»­500é”™è¯¯ï¼Œå·²é‡è¯•{max_attempts}æ¬¡ã€‚é”™è¯¯ç±»å‹: {error_type}ã€‚å»ºè®®: ç¨åé‡è¯•æˆ–æ£€æŸ¥APIé…é¢ã€‚åŸå§‹é”™è¯¯: {str(e)}")
                    
                    # å…¶ä»–ç±»å‹é”™è¯¯çš„ç‰¹æ®Šå¤„ç†
                    elif "rate limit" in error_msg or "quota" in error_msg:
                        logger.error(f"ğŸš« APIé…é¢æˆ–é€Ÿç‡é™åˆ¶é”™è¯¯: {str(e)}")
                        raise Exception(f"APIé…é¢ä¸è¶³æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹: {str(e)}")
                    
                    elif "timeout" in error_msg or "deadline" in error_msg:
                        logger.error(f"â° APIè¶…æ—¶é”™è¯¯: {str(e)}")
                        if attempt < max_attempts - 1:
                            delay = base_delay * (attempt + 1)  # çº¿æ€§å¢åŠ å»¶è¿Ÿ
                            logger.warning(f"ç½‘ç»œè¶…æ—¶ï¼Œ{delay}ç§’åé‡è¯•")
                            time.sleep(delay)
                            continue
                    
                    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                    raise
            
        llm_instance.invoke = enhanced_invoke
        return llm_instance
    
    def _create_ollama_llm(self, config: Dict[str, Any]) -> ChatOllama:
        """åˆ›å»ºOllama LLMå®ä¾‹"""
        ollama_config = {
            "model": config["model_name"],
            "temperature": config["temperature"]
        }
        
        # æ·»åŠ OllamaæœåŠ¡å™¨URL
        base_url = self.config.get("ollama_base_url", "http://localhost:11434")
        ollama_config["base_url"] = base_url
        
        return ChatOllama(**ollama_config)
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            æ¨¡å‹åç§°åˆ—è¡¨
        """
        return list(self.supported_models.keys())
    
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ¨¡å‹é…ç½®ä¿¡æ¯
        """
        return self.supported_models.get(model_name)
    
    def add_custom_model(self, model_name: str, config: Dict[str, Any]):
        """
        æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            config: æ¨¡å‹é…ç½®
        """
        required_fields = ["provider", "model_name", "temperature", "max_tokens"]
        for field in required_fields:
            if field not in config:
                raise ValueError(f"Missing required field: {field}")
        
        self.supported_models[model_name] = config
        logger.info(f"Added custom model: {model_name}")
    
    def clear_cache(self):
        """æ¸…ç©ºLLMç¼“å­˜"""
        self.llm_cache.clear()
        logger.info("LLM cache cleared")
    
    def validate_config(self) -> Dict[str, bool]:
        """
        éªŒè¯é…ç½®
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        validation_results = {
            "gemini_configured": bool(
                os.getenv("GOOGLE_API_KEY") or 
                os.getenv("GEMINI_API_KEY") or
                self.config.get("google_api_key") or 
                self.config.get("gemini_api_key")
            ),
            "ollama_configured": True  # Ollamaé€šå¸¸ä¸éœ€è¦å¯†é’¥
        }
        
        return validation_results
    
    def get_diagnostics_report(self) -> str:
        """è·å–è¯Šæ–­æŠ¥å‘Š"""
        diagnostics = get_global_diagnostics()
        return diagnostics.generate_diagnostic_report()
    
    def get_system_health(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        diagnostics = get_global_diagnostics()
        return diagnostics.get_health_status()
    
    def save_diagnostics_report(self, output_path: str = "gemini_diagnostics_report.md") -> str:
        """
        ä¿å­˜è¯Šæ–­æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        from .gemini_diagnostics import save_diagnostic_report
        return save_diagnostic_report(output_path)
    
    def reset_diagnostics(self):
        """é‡ç½®è¯Šæ–­æ•°æ®"""
        diagnostics = get_global_diagnostics()
        diagnostics.reset_stats()
        logger.info("ğŸ”„ LLMè¯Šæ–­æ•°æ®å·²é‡ç½®")